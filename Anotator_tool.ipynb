{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4YCuR6tc9qsr","executionInfo":{"status":"ok","timestamp":1725014999921,"user_tz":-180,"elapsed":30689,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}},"outputId":"d55fb476-8f0c-4fbe-9b65-17bacbf183c9"},"id":"4YCuR6tc9qsr","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install 'git+https://github.com/gebre138/tool.git'"],"metadata":{"id":"MAwV8uSU29We","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725015009011,"user_tz":-180,"elapsed":9103,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}},"outputId":"4c96799f-ecf7-49b7-aff0-2e7b817d5ecf"},"id":"MAwV8uSU29We","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/gebre138/tool.git\n","  Cloning https://github.com/gebre138/tool.git to /tmp/pip-req-build-yzfm0sro\n","  Running command git clone --filter=blob:none --quiet https://github.com/gebre138/tool.git /tmp/pip-req-build-yzfm0sro\n","  Resolved https://github.com/gebre138/tool.git to commit 729d572d8dcc3d7d82843191db53e10add8f22e1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: am_complexity\n","  Building wheel for am_complexity (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for am_complexity: filename=am_complexity-1.0-py3-none-any.whl size=4132 sha256=fd17dfe27a2609497b2c5ae353f97cf5538848ddad4709b5a22402c0573a87d0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-usg5hf7z/wheels/bf/1e/d0/67ddc9f7c06c7948005156e4a4c91d992e30b7e5689049e7da\n","Successfully built am_complexity\n","Installing collected packages: am_complexity\n","Successfully installed am_complexity-1.0\n"]}]},{"cell_type":"code","execution_count":3,"id":"Hhh8u4pZ0Xyt","metadata":{"id":"Hhh8u4pZ0Xyt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725015047310,"user_tz":-180,"elapsed":36568,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}},"outputId":"c77bf87f-4bb8-4574-f4f3-b45af0ef69f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting amseg\n","  Downloading amseg-2.3.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: amseg\n","  Building wheel for amseg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for amseg: filename=amseg-2.3-py3-none-any.whl size=9772 sha256=f4a873f95c807f45572d1d71b6267f7bdd88554f9922d8d0de014a8c8f719bd7\n","  Stored in directory: /root/.cache/pip/wheels/db/6b/cb/24ded8fb2b302a1040edc1dac7fe93f9cc0106ca610edccca9\n","Successfully built amseg\n","Installing collected packages: amseg\n","Successfully installed amseg-2.3\n","\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for glob\u001b[0m\u001b[31m\n","\u001b[0mProcessing ./drive/MyDrive/bert/tool/HornMorphoA-4.3-py3-none-any.whl\n","Installing collected packages: HornMorphoA\n","Successfully installed HornMorphoA-4.3\n","\n","@@@@ This is HornMorpho, version 4.3 @@@@\n","\n","No module named 'pytesseract'\n"]}],"source":["!pip install amseg\n","!pip install glob\n","!pip install '/content/drive/MyDrive/bert/tool/HornMorphoA-4.3-py3-none-any.whl'\n","\n","try:\n","    import os\n","    import pandas as pd\n","    import glob\n","    import re\n","    import hm\n","    import numpy as np\n","    from IPython.display import clear_output\n","    from collections import Counter\n","    from amseg. amharicNormalizer import AmharicNormalizer as normalizer\n","except ImportError as err:\n","    print(err)"]},{"cell_type":"code","execution_count":4,"id":"4mdTjHRJ0sdD","metadata":{"id":"4mdTjHRJ0sdD","executionInfo":{"status":"ok","timestamp":1725015048629,"user_tz":-180,"elapsed":1330,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}}},"outputs":[],"source":["os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n","\n","path = os.getcwd()\n","paths=\"\"\n","if path==\"/content\":\n","    paths=\"/content/drive/MyDrive\"\n","else:\n","    paths=path\n","if not os.path.exists(paths+\"/dataset\"):\n","    os.makedirs(paths+\"/dataset\")\n","if not os.path.exists(paths+\"/dataset/stoplist\"):\n","    os.makedirs(paths+\"/dataset/stoplist\")\n","if not os.path.exists(paths+\"/dataset/other\"):\n","    os.makedirs(paths+\"/dataset/other\")\n","\n","if not os.path.exists(paths+\"dataset/stoplist/spchar.txt\"):\n","    spch=\"á© á« áª á¬ á­ á® á¯ á° á± á² á³ á´ áµ á¶ á· á¸ á¹ á¶ áº á¯ á» á¼ 0 1 2 3 4 5 6 7 8 9 { } a A b B c C d D e E f F g G h H i I j J k K l L m M \\\n","    n N o O p P q Q r R s S t T u U v V w W x X y Y z Z '\"' | :\\ ; , . / < > ? [ ] ; , . / á¤ á£ á¢ á¡  â€ â€œ á  á¥ á¦ á§ á¨ \\ Â´ â€¦ \\\n","    !ã€Œ \"'\" Â¦ _ , \\ Â¨ á£ á¤ . á¹ á¢ ~ ! @ # $ % ^ & * ( ) _ + ` áŸ = - â€“ \\ufeff â€¢ â˜… ğŸ™‚ ï¿½ \"\n","    spch = spch.split()\n","    with open(paths+'/dataset/stoplist/spchar.txt', 'a',encoding=\"utf-8\") as file:\n","        for i in spch:\n","            file.write(i+\"\\n\")\n","\n","if not os.path.exists(paths+\"/dataset/stoplist/amharic_stop_lists.txt\"):\n","    amstop=\"á‹¨ áˆˆ á‰ á‹šáˆ… áŠ¥áŠ•á‹° áŠáŒˆáˆ­ áŠ áŠ•á‹µ áŠ áŠ•á‹µáŠ• áŠ¥áŠ“ áŠ áˆˆ áŠ á‹¨ á‹¨á‰µ áŒáŠ á‰ áˆ‹ áˆ†áŠ áˆˆá‹¨ á‰£áˆˆ áŒŠá‹œ áˆ„á‹° á‰  áŠ áˆ áˆƒ á‹« áŒ‹ áˆ†áŠ áŠáŒˆáˆ¨ áŠá‰ áˆ¨ á‹ˆá‹­áˆ áˆ†áŠ‘ áˆ†áŠ–áˆ áŠá‹ áŠ“á‰¸á‹ áŠá‰ áˆ­ áˆáˆ‰áŠ•áˆ áˆ‹á‹­ áˆŒáˆ‹ áˆŒáˆá‰½ áˆµáˆˆ \\\n","    á‰¢áˆ†áŠ• á‰¥á‰» áˆ˜áˆ†áŠ‘ áˆ›áˆˆá‰µ áˆ›áˆˆá‰± á‹¨áˆšáŒˆáŠ á‹¨áˆšáŒˆáŠ™ áˆ›á‹µáˆ¨áŒ áˆ›áŠ• áˆ›áŠ•áˆ áˆ²áˆ†áŠ• áˆ²áˆ áŠ¥á‹šáˆ… áŠ¥áŠ•áŒ‚ á‰ áŠ©áˆ á‰ á‹áˆµáŒ¥ á‰ áŒ£áˆ á‹­áˆ…áŠ• á‰ á‰°áˆˆá‹­ áŠ¥á‹«áŠ•á‹³áŠ•á‹µ á‰ áˆ†áŠ áŠ¨á‹šáˆ… áŠ¨áˆ‹á‹­ áŠ¨áˆ˜áˆ€áˆ áŠ¨áˆ˜áŠ«áŠ¨áˆ áŠ¨áŒ‹áˆ« áŒ‹áˆ« á‹ˆá‹˜á‰° \\\n","    á‹ˆá‹° á‹«áˆˆ áˆ²áˆ‰ á‰ á‰°áˆ˜áˆˆáŠ¨á‰° á‰ á‰°áˆ˜áˆ³áˆ³á‹­ á‹«áˆ‰ á‹¨áŠ‹áˆ‹ á‹¨áˆ°áˆáŠ‘  áˆáˆ‰ áˆáˆ‰áˆ áŠ‹áˆ‹ áˆáŠ”á‰³ áˆ†áŠ áˆ†áŠ‘ áˆ†áŠ–áˆ áˆáˆ áˆáˆ‰áŠ•áˆ áˆ‹á‹­ áˆŒáˆ‹ áˆŒáˆá‰½ áˆá‹© áˆ˜áˆ†áŠ‘ áˆ›áˆˆá‰µ áˆ›áˆˆá‰± áˆ˜áŠ«áŠ¨áˆ á‹¨áˆšáŒˆáŠ™ á‹¨áˆšáŒˆáŠ áˆ›á‹µáˆ¨áŒ áˆ›áŠ• \\\n","    áˆ›áŠ•áˆ áˆ°áˆáŠ‘áŠ• áˆ²áˆ†áŠ• áˆ²áˆ áˆ²áˆ‰ áˆµáˆˆ á‰¢á‰¢áˆ² á‰¢áˆ†áŠ• á‰¥áˆˆá‹‹áˆ á‰¥á‰» á‰¥á‹›á‰µ á‰¥á‹™ á‰¦á‰³ á‰ áˆ­áŠ«á‰³ á‰ áˆ°áˆáŠ‘ á‰ á‰³á‰½ á‰ áŠ‹áˆ‹ áŠ¥á‰£áŠ­áˆ… á‰ áŠ©áˆ á‰ á‹áˆµáŒ¥ á‰ áŒ£áˆ á‰¥á‰» á‰ á‰°áˆˆá‹­ á‰ á‰°áˆ˜áˆˆáŠ¨á‰° á‰ á‰°áˆ˜áˆ³áˆ³á‹­ á‹¨á‰°áˆˆá‹«á‹¨ á‹¨á‰°áˆˆá‹«á‹© \\\n","    á‰°á‰£áˆˆ á‰°áŒˆáˆˆáŒ¸ á‰°áŒˆáˆáŒ¿áˆ á‰°áŒ¨áˆ›áˆª á‰°áŠ¨áŠ“á‹áŠ—áˆ á‰½áŒáˆ­ á‰³á‰½ á‰µáŠ“áŠ•á‰µ áŠá‰ áˆ¨á‰½ áŠá‰ áˆ© áŠá‰ áˆ¨ áŠá‹ áŠá‹­ áŠáŒˆáˆ­ áŠáŒˆáˆ®á‰½ áŠ“á‰µ áŠ“á‰¸á‹ áŠ áˆáŠ• áŠ áˆˆ áŠ áˆµá‰³á‹ˆá‰€ áŠ áˆµá‰³á‹á‰€á‹‹áˆ áŠ áˆµá‰³á‹áˆ°á‹‹áˆ áŠ¥áˆµáŠ«áˆáŠ• áŠ áˆ³áˆ°á‰  áŠ áˆ³áˆµá‰ á‹‹áˆ \\\n","    áŠ áˆµáˆáˆ‹áŒŠ áŠ áˆµáŒˆáŠá‹˜á‰¡ áŠ áˆµáŒˆáŠ•á‹á‰ á‹‹áˆ áŠ á‰¥áˆ«áˆ­á‰°á‹‹áˆ áŠ¥á‰£áŠ­á‹ áŠ áŠ•á‹µ áŠ áŠ•áŒ»áˆ­ áŠ¥áˆµáŠªá‹°áˆ­áˆµ áŠ¥áŠ•áŠ³ áŠ¥áˆµáŠ¨ áŠ¥á‹šáˆ áŠ¥áŠ“ áŠ¥áŠ•á‹° áŠ¥áŠ•á‹°áŒˆáˆˆáŒ¹á‰µ áŠ¥áŠ•á‹°á‰°áŒˆáˆˆáŒ¸á‹ áŠ¥áŠ•á‹°á‰°áŠ“áŒˆáˆ©á‰µ áŠ¥áŠ•á‹°áŠ áˆµáˆ¨á‹±á‰µ áŠ¥áŠ•á‹°áŒˆáŠ“ á‹ˆá‰…á‰µ áŠ¥áŠ•á‹²áˆáˆ \\\n","    áŠ¥áŠ•áŒ‚ áŠ¥á‹šáˆ… áŠ¥á‹šá‹« áŠ¥á‹«áŠ•á‹³áŠ•á‹± áŠ¥á‹«áŠ•á‹³áŠ•á‹³á‰½á‹ áŠ¥á‹«áŠ•á‹³áŠ•á‹· áŠ¨ áŠ¨áŠ‹áˆ‹ áŠ¨áˆ‹á‹­ áŠ¨áˆ˜áŠ«áŠ¨áˆ áŠ¨áˆ°áˆáŠ‘ áŠ¨á‰³á‰½ áŠ¨á‹áˆµáŒ¥ áŠ¨áŒ‹áˆ« áŠ¨áŠá‰µ á‹ˆá‹˜á‰° á‹ˆá‹­áˆ á‹ˆá‹° á‹ˆá‹°áŠá‰µ á‹áˆµáŒ¥ áŠ¥á‰£áŠ­áˆ¸ á‹áŒª á‹«áˆˆ á‹«áˆ‰ á‹­áŒˆá‰£áˆ á‹¨áŠ‹áˆ‹ á‹¨áˆ°áˆáŠ‘ \\\n","    á‹¨á‰³á‰½ á‹¨á‹áˆµáŒ¥ á‹¨áŒ‹áˆ« á‹« á‹­á‰³á‹ˆáˆ³áˆ á‹­áˆ… á‹°áŒáˆ á‹µáˆ¨áˆµ áŒ‹áˆ« áŒáŠ• áŒˆáˆáŒ¿áˆ áŒˆáˆáŒ¸á‹‹áˆ áŒá‹œ áŒ¥á‰‚á‰µ áŠá‰µ á‹°áŒáˆ á‹›áˆ¬ áŒ‹áˆ­ á‰°áŠ“áŒáˆ¨á‹‹áˆ á‹¨áŒˆáˆˆáŒ¹á‰µ á‹­áŒˆáˆáŒ»áˆ áˆ²áˆ‰ á‰¥áˆˆá‹‹áˆ áˆµáˆˆáˆ†áŠ áŠ á‰¶ áˆ†áŠ–áˆ áˆ˜áŒáˆˆáŒ¹áŠ• áŠ áˆ˜áˆáŠ­á‰°á‹‹áˆ \\\n","    á‹­áŠ“áŒˆáˆ«áˆ‰ áŠ á‰ áˆ«áˆ­á‰°á‹ áŠ áˆµáˆ¨á‹µá‰°á‹‹áˆ áŠ¥áˆµáŠ¨ á‹­áˆ… áŠ¨áŠ á‹«áˆˆ á‹ˆá‹° áˆµáˆˆ á‰°áˆ« áˆ™áˆ‰ áŒ‹áˆ­ áŠ¥áŠ“ áŠá‹ áŒáŠ• á‹ˆá‹­áˆ áŠ¥áŠ•áŒ… áŠ¥áŠ•áŠ³ áŠ“á‰¸á‹ áŠ á‹áŠ• áŠ¥áŠ•á‹²áˆ… áŠ¥áŠá‹šáˆ… áˆáŠ• á‹­áŠ¸á‹áˆ\"\n","    amstop = amstop.split()\n","    with open(paths+'/dataset/stoplist/amharic_stop_lists.txt', 'a',encoding=\"utf-8\") as file:\n","        for i in amstop:\n","            file.write(i+\"\\n\")\n","\n","if not os.path.exists(paths+\"/dataset/other/complex_word.xlsx\"):\n","    root=\"áŠ®áˆµáˆ›áŠ“ áˆ˜á‰ƒáˆ­ á‹ˆá‹°á‰¥ á‰†áˆ¨áˆá‹° áˆˆáˆáŒ½ á‹°á‰¦ áˆˆáˆ°áˆ° áˆˆáˆ°áŠ áˆˆáˆ´ áˆˆá‰†á‰³ áˆˆá‰ á‰… áˆˆá‰ áŠ• áˆˆá‰ á‹° áˆˆá‰°á‰° áˆˆáŠ¨á‰µ áˆˆáŠ°áˆ áˆˆá‹˜áˆˆá‹˜ áˆˆá‹˜á‹˜ áˆˆáŒˆáˆ˜ áˆˆáˆáˆ áˆá‰£á‰¥ áˆ˜áŠá‹°áŒˆ áˆáŒˆá‹µ áˆáŒá‹šá‰µ áˆ˜áˆáŒ¨áŒ­ áˆ¨áˆ˜áŒ¥ áˆ¨á‰€á‰€ \\\n","    áˆ¨á‰¥áŒ£ áˆ˜á‰ƒá‰ƒáˆ­ áŠ¥áŠ•áŒ­áŒ­ áŒˆáŠáŠ áˆ­á‹°á‰µ áˆ­áˆµá‰µ áˆ­á‰±áŠ¥ áŠ áˆ­áŠ¥áˆµá‰µ áˆ®á‰„ áˆ°áˆˆá‰€ áˆ°áˆˆá‰  áˆ°áˆˆáŒ  áˆ°áˆ‹áŒ¤ áˆ°áˆ¨áŠá‰€ áˆ˜áˆµáˆ¨áŒ áˆ˜áˆµáˆ¨áŒ½ áˆ°á‰€áŒ  áˆ°á‰ á‰€ á‰°áˆ°áŠ“áˆ°áˆˆ áˆ°áŠá‰ áŒ  áˆ°áŠ•á‰ áˆ­ áˆ´áˆ« áˆµá‹µ áˆ°áŒ‹áˆ­ áˆ°áŒˆáŠá‰µ áˆ°áŒá‹³á‹³ áˆ¨á‰£á‹³ áˆ²áˆ«áŠ­ \\\n","    áˆ²áˆ³á‹­ áˆ³áŠ•á‰ƒ á‰£á‹­á‰°á‹‹áˆ­ áˆ´áˆ°áŠ› áˆˆá‰€áˆˆá‰€ áˆ‹áˆ¸á‰€ á‹ˆáˆ¨á‰µ á‰…á‹­áŒ¥ áŒ‹áˆ¬áŒ£ áˆ˜áˆ›áˆµ áŒˆáˆ¨áˆ¨ áŠ¡á‹°á‰µ áá‹­á‹ áŠ áˆ¸áˆˆá‰  áˆ¸áˆˆá‰° áˆ¸áˆ˜á‰€ áˆ¸á‰€áŒ  áˆ¸áˆáŒ  á‰€áˆˆáˆ° á‰€áˆ¸áˆ¨ áŠ¨áˆ¸áŠ á‰€á‰°áˆ¨ á‰€áˆ°áˆ¨ á‰€áŠáŒ áˆ° á‰€áŠáŒ¨áˆ¨ á‰€áŠ–áŠ“ áˆˆá‰ áŒ  á‹°áŒáˆ° áŒáˆ« \\\n","    áˆ˜áˆµáŠ­ áˆ˜áˆ¨áŠ• áˆ°á‹¨áˆ˜ á‰°áˆ¾áˆ˜ á‰‹áˆ³ á‰£á‰°áˆŒ á‰£á‹áˆ« á‰°áˆ°á‰ áŒ£áŒ áˆ¨ áŠáˆáŒˆ á‰°á‹°áˆ«áˆ² áˆáŠ“á‰¥ áŠ á‹°áˆ˜ áŠ•á‹‹á‹­ áŠ•á‰áŒ áŠ áˆˆáˆ˜ áŠ áˆˆá‰£ áˆ›áˆ˜áŠ•á‹ áŠ­ áŠ áˆšáŠ«áˆ‹ áŠ áˆá‰£áˆ¨á‰€ áˆ˜áŠá‹˜áˆ¨ áˆ˜áŒ¥áŠ” áˆ€áˆ©áˆ­ áŠ áˆá‰£ á‰…áˆ­áŠá‰µ á‹°áˆˆáˆ áŠ­áˆá‰½á‰µ áˆ¨á‰‚á‰… \\\n","    áŒ¥á‰…áŒ¥á‰… áŠ¥áŠ•á‰ áŠ á‰€á‰  áˆˆá‹°áˆ á‰‹á‰µ áŠ áŠ¨áŠ•á‹áˆ½ áŒá‹µá‹áˆª áˆ»á‰€áˆˆ áˆ¸áŠ¨áŠ á‹ˆáŒ£áˆ« áˆ°á‹‹áˆ« áŠ áˆáŠ«áˆ¸ á‰°áˆ˜áˆ˜ áŒáŒ áŠ á‹°á‰¥ áŠ©á‰³áŒˆáŒ áˆ á‹ˆáˆ¨á‹› á‹ˆá‰ á‰… á‹°áŒ“áˆ³ áŒ…á‰¥áˆ« áŒ¥áŒˆá‰µ á‹á‹­á‹³ áˆ€áŒˆáˆ¨áˆ°á‰¥ áˆáˆˆáŠ•á‰°áŠ“ áˆáŠáŠ› áˆƒáˆŒá‰³ áˆ€áˆ˜áˆáˆ›áˆ \\\n","    á‹­áˆµáˆ™áˆ‹ áˆ‰áŠ áˆ‹á‹Š áˆáŒ‹ áˆ‰áŒ¤ áˆáˆ«áŒ… áˆ€áˆ´á‰µ áˆ˜áˆ³ áˆ›áŠ¥áˆ¨áŒ áˆáŠ•á‹áˆ­ á‰ƒáŠ•á‹› á‰ áˆˆá‰°á‰° á‰¡ááŠ“ á‰¦áˆ¨á‰€ á‰§áˆá‰µ áˆ˜áŒ‹á‹¨á‰µ á‰£áˆ‹áŠ•áŒ£ á‰†áˆ¨á‰†áˆ° áŠ¥áˆá‰£á‰µ á‰°áŒáˆ³áŒ½ á‰°áŒ áŠ“á‹ˆá‰° á‰¸áˆá‰°áŠáŠá‰µ á‰¸áŠáˆáˆ­ áŠáˆ¸áŒ  áŠá‹áŒ¥ áŠá€á‰¥áˆ«á‰… áˆ¨á‰ áˆ¨á‰  \\\n","    áŠ áˆ¨áˆá‰€ áŠ áˆ­áˆáˆ áŠ á‰°áˆ˜ áŠ á‹˜á‰¦á‰µ áŠ áŒˆáŠ“ áŠ áŒá‰¦áŠ› áŠ áŒ­áˆáŒ áŠ¢áˆáŠ•á‰µ áŠ¥á‰…áŒ©áŠ• áŠ áŠ¨á‰°áˆ˜ áŠ¥á‹áŠ• áŠ¥áŠ•á‰¦á‰€á‰…áˆ‹ áŠ¨á‹ˆáŠ á‹ˆáˆ¨áŒá‰¡ á‹šá‰€áŠ› á‹›á‰ áˆ¨ á‹áˆ› á‹­á‹ á‹°áˆ¨áˆ˜áŠ• á‹µá‹µáˆ­ áŒ€áˆŒ áŒˆáˆ­áŒƒá‹ áŒáŠá‰†áˆˆ á‰°áŒáˆ³á‰†áˆˆ áŒ¥áˆáŠ“ áŒ¨áˆ¨áá‰³ \\\n","    áŒ­á‰¥áŒ¥ áŒ¸á‹³áˆ áŒ½áˆáˆ˜á‰µ á‹ˆáŠ“ á‹ˆáˆ› áŒ½áŒŒ áˆá‰°á‰° áˆáŒˆáˆ˜ áˆáŒˆáŒ  ááˆ­áŠ©á‰³ áŒáˆ¬ ááŒ áŠ“ áˆ€áŒ« áˆ€áŠ¬á‰µ áˆá‹³á‹µ áˆ…áˆ‹á‹Œ áˆ…á‰¡á‹• áˆˆá‹áˆ‹á‹‹ áˆˆáˆ†áˆ³áˆµ áˆˆá‰†áŒ  áˆŒáŒ£ áˆáŒ¨áŠ› áˆá‰€á‰µ áˆáŒ¥ áˆ˜áˆ°áˆª áˆ˜á‰ƒáŠ áˆ˜áˆ¨áŠ€ áˆ˜á‰…áŠ– áˆ˜áŠá‹˜áˆˆ \\\n","    áˆ™á‹³ áˆ˜áˆ­áŒ áˆ˜á‹µá‰¥áˆ áˆ˜á‰ áˆˆá‰µ áˆ™áŠ“ áˆáŒ¸á‰µ áˆ›áˆ­áŒ£ áˆ›áá‹³ áˆ˜á‹¨áˆ° áˆŸáŒ¨ áˆ¨á‰ á‰  áˆ¨á‰¥ áˆ˜áŒ«á‰µ áˆ«á‹° áˆµáŠ“á‹³áˆª áŒá‹á‰µ á‰°áˆ°áŠá‰€ áˆ°áŠá‰€ á‹µá‰¥áŠá‰µ á‰€áˆ¨áˆ¨ áŠ¨áˆ­áˆµ áŠ áˆá‹µ áˆ€áˆ³á‹Š á‰€áŒ‹ áˆ¹áˆ áˆƒáˆ³á‹Š áˆ½á‰¥áˆá‰… áˆ¸áŠá‰†áŒ  \\\n","    áˆ¾áˆ˜áŒ áˆ¨ áˆ»áŒ‰áˆ« áˆ¾á‰°áˆ áˆ¸ááŒ¥ á‰€áˆá‰¥ á‰…áˆáŒ¥áˆ á‰€áŠ•áŒƒ á‰€á‰ áŠ› á‰€á‹¨á‹° á‰áŠ•á‹³áˆ‹ á‰áŠ•áŒ½áˆ á‹áˆ­áŒ­ á‹áˆ€ á‰°áŠ“áŒ‹ áŒ áŠ” áŒ‰á‹« á‰°áŒá‹³áˆ®á‰µ á‰áŠ•áŒ£áŠ• á‹ˆáˆá‰… áŠ“áŒ  áˆµáˆá‰³ áŠ¨áŒ€áˆˆ áˆ˜áŒˆáˆ­áˆ˜áˆ áŒ‰áˆ«áˆ›á‹­áˆŒ á‹°áˆá‰ƒá‰ƒ áŒ½áŠ•áˆáŠ› áˆ˜á‰°á‹¨á‰¥ \\\n","    áˆ…áŒ¸áŒ½ áˆŸáˆ­á‰µ áˆŒáˆ›á‰µ á‹™á‹áŠ• áˆáˆµá‰…áˆáŠ“ á‹ˆáˆˆáˆ ááˆ­áˆáˆ« áˆ˜áŠ– áŠ áˆ½áŠ¨áˆ­ á‰½áˆ®á‰³ áˆáŒ¨áˆˆáˆ á‰€áˆˆá‰ áˆ° áˆ¸áˆˆá‰€á‰€ áˆ¸áˆ˜áŒ áŒ  á‹˜áŠáŒ áˆˆ á‰¸áŠáŠ¨áˆ¨ áˆáŠá‹°á‰€ á‹˜áˆ¨áŒ áŒ  á‹ˆáˆ°áˆˆá‰° áŠ•á‰µáˆ­áŠ­ áˆ°áˆˆá‰€áŒ  áˆ­áˆšáŒ¦ áŒ áŒ áˆ¨ á‰°áŒáŠáŒ¨ á‰°á‹°áˆ˜áˆ˜ \\\n","    áŠ‘á‰£áˆ¬ áŠ áˆœáŠ¬áˆ‹ áŠ áˆ½áŠ­áˆ‹ áŠ áˆ½áŠ«áŠ« áˆ˜áˆ³áˆˆáŒ¥ áŠ¥á‰¥áˆªá‰µ áˆˆá‹‹áˆ³ áŠá‹› áˆáŠ¨á‰µ áˆˆáˆá‰¦áŒ­ áˆŠá‰… áˆŒáŒ¦ áˆáˆ»áŠ• áŠ¥áŒ­ áˆá‹á‰ áˆ« áˆ˜á‹˜á‹ áˆ˜á‹²áŠ“ áˆ˜áŒáˆ‹áˆŠá‰µ áˆáŒ¥á‹‹á‰µ áˆ™áˆ¬ áˆ™áˆ¾ áˆ›áˆ­á‹³ áˆ›á‰… áˆ›áŠ¥á‰€á‰¥ áˆ›áŠ¥á‹˜áŠ• áˆ›áŒ áˆ›áŒ¥ áˆáˆ…á‹‹áˆ­ \\\n","    áˆáŠ­áˆ¸ á‹˜áˆ€ áˆµáˆ­á‹ˆ áˆµá‹áˆ­ áˆ­á‹áˆ«á‹¥ áˆ¨áŒáˆ¨áŒ áˆªá‹ áŠ¥áˆ­á‰ƒáŠ• áˆ¬á‰µ áˆ®áˆ® áˆ°áŠá‹µ áˆ°áˆáŠ áˆ²á‰£áŒ áˆ³á‹±áˆ‹ áˆµáˆá‰µ á‹áˆ á‰£áˆáŠ•áŒ€áˆ« áˆ˜áˆ½áˆˆá‰µ áˆ½áˆ­á‹°á‹³ áˆ¸áˆ¸ áˆ¸á‰€áˆˆ áˆ½áŠ•áŒˆáˆ‹ áˆ¸áŠáˆ¸áŠ áŒ…áˆ¨á‰µ áˆ¸áŠ•áŒ áˆ»áŒˆá‰° áˆ…á‰¥áˆ­ á‰€áˆˆá‰¥ áˆ˜á‰€áˆá‹ˆáŒ¥ áŠ®áˆ¨á‹³ \\\n","    á‰…áˆµáˆá‰µ á‰…áˆ«áˆª áŠ•áŒ¥áˆ­ á‰ƒáˆ¨áˆ˜ á‰…áŠ•áŒ£á‰µ á‹áŒ¥áŠ•á‰…áŒ¥ á‰‹áŒ¥áŠ á‰¢áŒ¤ á‰¥áˆ‹áˆ½ á‰¥áˆ‹á‰´áŠ“ á‰¡áŒ«á‰‚ á‰µáˆáˆ á‰°áˆ˜áŠ• á‰°áˆ¨á‰¥ á‰€á‹áˆµ áŠ•áˆ¨á‰µ áˆ˜á‰°á‰¸á‰µ áŠ«áŠ á‹ˆáŠ¨á‰£ áˆ˜á‹˜áŠ¨áˆ­ áŠ¥áˆ­á‰… á‰µá‰¢á‹« á‰µáˆ­áˆáˆµ á‰¥áŒ£áˆª áŒˆá‰ áˆ¨ áŒ‰á‰¦ áŒá‰µáˆ­ áŒˆá‹µ \\\n","    á‹˜áŒˆáˆá‰°áŠ› áŒ­ááˆ« áˆ½áŒáŒáˆ­ áŒ‰áˆáˆ‹á‰µ áŒáˆ½á‰ á‰µ áˆ˜áŠ¨á‰³ áŒá‰¥á‹ áŒáŠ¡á‹ áŒáˆ°á‰ˆáˆˆ áŒ¥áˆ­áŠ á‰£áˆˆáˆŸáˆ á‰£áˆá‹°áˆ¨á‰£ á‰µáŠ«á‹œ á‰µáŠ¥á‹­áŠ•á‰µ áŒá‰¥áŠ á‰µ áˆ˜áŠá‰£áŠ•á‰¥ áŠ áˆ‰á‰£áˆá‰³ áŠ áˆ‰á‰³ áŠ á‹ˆáŠ•á‰³ áŠ áˆ³áˆ­ áŠ áˆ­áŒ©áˆœ áŠ áˆ»áˆ« áŠ áˆ½áˆ™áˆ­ áŠ á‰ áˆ \\\n","    á‹³á‰£ áŠ á‰¦áˆ áŠ¥áŠ­áˆ áˆ›áŠá‰† áŠ¥áŠ•áŒá‰» áŠ•áŠ¡áˆµ áŠ áŠ­ááˆá‰°áŠ› áˆáŠ«á‰³ áŠ á‹°áˆˆ áŠ ááˆ‹ áŠ¥áˆááŠ á‹ˆáŒ áŠ á‹áˆ‹áˆ‹ áŠ¥áˆá‰¡áŒ¥ áŠ¥áˆá‰… áŠ¥áˆµáˆµá‰µ áŠ¨áˆ°áˆ˜ áŠ¬áˆ‹ áŠ®áˆ¨á‰¥á‰³ á‹ˆáˆ‹áˆáŠ• á‹á‰…áˆ«á‰µ áˆ˜á‹‹á‰…áˆ­ á‹ˆáŠ•á‰ á‹´ á‹ˆáŠ•á‹°áˆ‹áŒ¤ á‹ˆá‹˜áŠ“ á‹ˆáŒ€á‰¥ \\\n","    á‹ˆáŒŒáˆ» á‹˜á‹¨áˆ¨ áˆ˜á‹áˆá‰… á‹µáˆá‹ á‹°áˆá‰¥ á‹µáˆ­á‹µáˆ­ á‹°á‰£ á‹°á‰£áˆ á‹°á‰¥áˆ­ á‹°á‰¥á‹› á‹°áŠ•á‰³ á‹°á‹Œ á‹°áŒ€áŠ• á‹µáŒáˆ› á‹°áˆáˆ¨áˆ° á‹±á‰¤ á‰€áˆ˜áˆ˜ á‹³á‹‹ á‹µáŠ•áŠ­ áŒˆá‰£áˆ­ áŒáˆµá‰‹áˆ‹ áŒ áŠ•á‰… á‰°á‹°áˆˆá‰€ áˆ˜áˆ‹á‹ˆáˆµ á‰£áˆˆáŒ áŒ‹ áŒ­áˆ‹áŠ•áŒ­áˆ áŒ¸áŠ‘ áŒ½á‹‹ ááˆáˆ°á‰µ \\\n","    áˆáŠáŒ á‹˜ áˆá‹áˆµ ááŠ«á‰µ á‹áˆµáŠ® á‹áŠ–áˆµ áŒá‹ ááˆµáˆ€ á‰ áˆ¨á‰ áˆ¨ á‰µá‰½á‰µ áŠ á‰ áˆ³ áŠ áˆˆáŠá‰³ áŠáŒá‹´ áŠ á‹ˆáŠ¨ áˆ¸áˆ­áŠ­ á‰µáŠ•á‰ á‹« áŠ áŠ•áŒ“ á‰€áˆ¨á‰€áˆ¨ á‹‹áˆá‰³ áˆ…á‹³áŒ á‹«áˆ¸á‰ áˆ¨á‰€ áˆ°áˆ­áŒ¥ áŒ‰á‰¶ áˆ˜áŠáˆ˜áŠ á‰€á‹® á‹ˆá‰µáˆ® á‰£á‹á‹› á‹˜áˆ¨áˆ¨ á‹²á‰ƒáˆ‹ á‰¦áŒ­á‰§áŒ« \\\n","    á‰†áŒ­á‰‹áŒ« áˆ½áˆ™áŒ¥ áˆ°áˆ°áŠ á‰°áˆáŠ“áˆáŠ áŒ‹áŒ  áŠ¨áŠáŠ¨áŠ áŠ áŠ•áŠ³áˆ­ á‰°áˆ˜áˆ³á‰€áˆˆ áŠ á‹ˆáŒˆá‹˜ áŒˆáˆ­ áŒˆá‰³ áˆ°áŠáŠ¨áˆˆ á‰°áŠ“áŒ  áŒáˆ…á‹°á‰µ áˆ›áˆ¾áˆ­ áˆ˜á‹áŒˆáŠ• á‹ˆáˆ¨á‰³ áŒˆáˆ€á‹µ áŠ áˆ­á‰³áŠ¢ áŒˆáˆá‰µ áŠ áŒ€á‰¥ áˆ…á‰¡áŠ¥ á‰‹áŒ¨ á‰µáŒ‰ áˆ¨áŒˆá‹µ á‹³á‹´ áˆ˜áŠ¨á‰° á‹ˆáˆ¨áˆ° \\\n","    áˆ­áˆ…áˆ©áˆ… áŒ¥áˆá‰… á‰°áŒˆá‰£á‹°á‹° á‹ˆá‰€áˆ³ á‰€áŠ•á‰ áˆ­ áˆˆáŒˆáˆ° áˆáˆ½áŒ áŠ áŠ•áŒ‹á‹³ áˆ˜áŠ“á‹ˆá‹ áˆ˜áˆáŠáˆ½áŠáˆ½ á‰°áŠ•áŒ£áŒ£ á‹á‹ á‹˜áŠ¨áˆ¨ áŠ áˆ› á‰³á‰€á‰  áŠ áŒˆá‰° áˆ˜áŠá‰€áˆ¨ áˆ°áˆ¨áŒ¸ áŠ áˆ¾á‰€ áŒ¥áˆ» áˆ›áŠ› áˆ…áˆá‹áŠ“ áˆá‰… áˆˆá‹˜á‰  á‰°áˆ˜áˆ¨á‹˜ áŠ áˆáˆ³áˆ áˆ˜áˆ²áŠ“ áˆ˜á‰ƒ \\\n","    áˆ˜áŠ“áŠ áˆ˜áŠ«áŠ• áˆ›áŒˆáŒ  áˆµáˆá‰» áˆ°áˆ˜áˆ¨ áˆµáˆµ áˆ±á‰£áŠ¤ áˆµáŠ•á‹± áˆµáŒ‹á‰µ áŠ áˆ°áŒ‹ áˆ¾áˆˆ áˆ½áŠ•áˆ½áŠ• á‰‚áˆ áˆ»áŠ•á‹³ á‰€áˆ‹á‹µ á‰†áˆ¨á‰†áˆ¨ á‰†áˆ¨á‰†á‹˜ á‰…áŠá‰µ á‰¥áˆ„áˆ«á‹Š á‰£áˆá‰¦áˆ‹ á‰£áˆá‰´á‰µ á‰¥áˆ©áˆ… á‰£áˆ­áŠ”áŒ£ áŠ á‰ áˆ°áˆ¨ á‰¦á‰ƒ á‰ áŠáŠ á‰£áŠáŠ á‰ á‹ˆá‹˜ á‰ á‹³ á‰§áŒ áŒ  \\\n","    á‰µáˆá‰µ á‰³á‰ á‹¨ á‰³á‰³áˆª á‰µáŠ¥á‰¢á‰µ á‰³á‹› á‰µá‹á‰¥á‰µ á‰°á‹µáˆ‹ á‰°áŒˆáŠ• áˆ˜áŠ•á‰ áˆ­ áŠ áˆˆá‰ áŠá‰  áŠ áŠ“á‹ˆáŒ  áŠáŒ á‰  áŠáŒ áˆ áŠ á‹á‹µ áˆáŠ¥áˆ‹á‹µ áˆáŠ¥áˆ˜áŠ• áŠ¥áˆ™áŠ• áŠ¥áˆ˜áŒ«á‰µ áŠ áˆ¨áˆ˜áŠ” á‹ˆáŒˆáŒá‰³ á‹‹á‰¢ á‹˜á‰ á‰µ áˆˆáŒáŒ¥ á‰¦á‰°áˆ¨áˆ á‹˜á‰€áŒ  á‰°á‹›áˆ˜á‰° \\\n","    á‹˜áˆˆáˆ° á‹˜áˆˆáˆ á‹˜áˆ‹á‰ á‹° áŠ á‹‹á‹› á‹áˆ½áŠ•ááˆ­ áŠ á‹á‰³áˆ­ áŠ áˆ¸á‰ áˆ¨á‰€ á‹˜áˆ¨áŒ¦ á‰€áˆ‹áˆ˜á‹° áŠ¨áŠá‰ áˆˆ á‹ˆáˆ¨áŠ•áŒ¦ á‹ˆáˆ˜áŠ” áŠ¨á‹­áˆ² á‹ˆáˆ®á‰ áˆ‹ á‹ˆáˆ¨áŒ‹ áŠ¨áŠá‰¸áˆ¨ áŠ¨áŠáˆ áŠ¨á‰°á‰  á‹ˆáˆ°áŠ« á‹ˆá‹­á‰£ á‹ˆáŒ áŠ á‹ˆáŒˆáŠ á‹ˆáŒˆáˆ¨ áŠ¨áˆ˜áŠ¨áˆ˜ áŠ¨áˆ¨á‹°á‹° áŠ¨áˆá‹‹áˆ³ áŠ¨áˆ‹á‰£ \\\n","    á‹ˆá‹˜áˆ á‰°á‹ˆáŠáŒ¨áˆ á‹ˆáŠ” á‹ˆá‹°áˆ˜ áŠ á‰†áˆ«áŠ˜ á‹‹áˆˆáˆˆ áˆ¸á‰€áˆ¸á‰€ áŠ¥áá‹­á‰³ áŠ¨áˆˆá‰  á‰¸áˆ­ á‹á‹á áˆˆá‹˜á‰¥á‰°áŠ› á‰€áˆ³áˆ› á‰µáˆáŠ­áˆ…á‰µ áŒ¥á‰ƒá‰…áŠ• áˆ€á‰°á‰³ áˆ‚áˆµ áŒ á‰ á‰¥á‰µ áˆáˆ³áŠ• áˆáˆŒ áˆ˜áˆ°á‹‹á‰µ áˆ˜áˆµá‹‹á‹•á‰µ áˆ˜á‰£ áˆ˜áŠ•áŒ‹ áˆ˜á‹°á‹³ áˆ˜á‹µáŠ• \\\n","    áˆ›áˆ³ áˆ›áŠ¥á‰ áˆ áˆ°áˆˆá‰£ áˆ½áˆ˜á‰ƒ á‰¤á‹› áŠá‰ áˆá‰£áˆ áŠ áˆ‹á‰£ áŠ á‰€á‰ á‰µ áŠ á‰» áŠ áŠ•áŒ‹á‹ áŠ á‹µáˆ› áŠ áŒ‹á‹áˆª áŠ áˆáŠ•áŒ‹áŒ­ áŠ­áˆ­á‰³áˆµ áŠ­áˆá á‹‹áˆµá‰µáŠ“ á‹‹áŒ á‹áŒ¥áŠ• á‹µáŠ•áŒ‹áŒŒ áŒ¥áˆªá‰µ áˆáˆ­ ááŠ•áŒ­ áˆ°áŒ‹ á‰ƒáŠ˜ á‰°áˆ³áŠ á‹˜áŠáŒ‹ á‹›áˆˆ á‹°áŠáŒˆáŒˆ á‰°áŠ•áŒ¸á‰£áˆ¨á‰€ \\\n","    á‰°áŠ¨áˆ›á‰¸ á‰°áŠ¨áˆ°á‰° á‰°á‹‹á‰€áˆ¨ áŒ€áŠ•áˆáˆ á‹ˆáŒ‹á‹³ áˆ°áŒˆá‰£ áˆ°á‰ áŠ¨ áˆ°áŒˆáˆ°áŒˆ á‰€áˆˆáˆ˜ áŠ“áˆ™áŠ“ á‰µáˆ­áŠ¢á‰µ áŠ¥áŒ© áˆ¸áˆ¨áŠ› áŠ¥áŠ•á‹áˆ‹áˆ áŠ¨áˆ¨áŠ¨áˆ° á‹á‹µáˆ› á‰°á‹˜áŠ¨áˆ¨ á‹˜áŠ¨á‹˜áŠ¨ á‹˜á‹á‹µ á‹˜á‹­á‰¤ á‹›á‰» á‹°áˆˆáˆ° á‹°á‰ƒ á‹°áˆˆáˆ˜ á‹°áˆˆá‰  á‹°áˆˆá‹˜ á‹µáˆªá‰¶ á‹µáˆª \\\n","    á‹°áˆ°á‰€ á‹°á‰ á‰° á‹°á‰€áŠ á‹°áŒˆáŠ á‹°á‰€á‹°á‰€ á‹°á‰ áˆˆ á‹°á‰£á‹­ á‹°áŠá‰ á‹˜ á‹°áŠá‹ á‹°áŒˆáˆˆáˆˆ á‹µáŠ•áŒ‰áˆµ á‹°áˆá‰€ á‹°áˆáŒ áŒ  á‹³áˆ° á‹°áˆáˆ­ á‹³áˆ¸á‰€ á‹³á‰ áˆ¨ á‹³á‰ áˆ° á‹³á‰°áŠ› á‹³áŠ¨áˆ¨ áˆ‹á‰†áŒ  áŒáˆµá‰³ á‹µáŠ•á‹á‰³ á‹¶áˆˆá‰° áŠ á‰ƒáŒ£áˆª áŒ…áˆáˆ‹ áŒ€áˆá‰ áˆ­ áŒ€á‰¥á‹µ áŒƒáŒ€ \\\n","    áŒˆáˆˆáˆ˜áŒ  áŒˆáˆˆáˆáŒ  á‰°áŒˆáˆ›áˆ¸áˆ¨ áŒˆáˆ¨á‹˜á‹˜ áŒˆáˆ¨áŒˆáˆ¨ áŠ áˆ˜áŒ¸ áŒˆáˆ°áŒˆáˆ° áŒˆáˆ°áŒ¸ áŒˆáˆ¸áˆˆáŒ  áŒˆáŠá‰°áˆ¨ áŒáˆ‹ áŒˆáŒ áŒ  áŒá áŒá‹™á áŒáˆ¨áŒ  áŒáˆ¨áŒáˆ¨ áŒáˆ°á‰†áˆˆ áŒáŠáŒ  á‰°áŒáŠ“áŒ¸áˆ áŒáˆáŠáŠ áŒáˆá‹¨ áŠ áŒ“áˆ« áŠ áŒáˆ¨ áŒ áˆ€áŠ˜ áŒ áˆˆáˆˆ áŒ áˆˆáˆ˜ áŒ áˆáˆ°áˆ á‰°áŒ¥áˆˆá‰€áˆˆá‰€ \\\n","    áŒ áˆˆá‹˜ áŠáˆ¨á‰° áŒ áˆ¨áˆ¨ áŒ áˆ¨áˆ˜áˆ° áŒ áˆ¨áŠá‰€ áŒ áˆ¨áˆ á‰€áˆ¨á‰€á‰  áŒ áˆ°áŒ áˆ° áŒ á‰¢á‰¥ áŒ áŠá‰ áˆ° áŒ áŠáŠ áŒ áŠá‹› áŒ áŠáŒ áŠ áŒ áŠáˆáˆ áŒ áŠ“ áŒ½áŠ“á‰µ áŠ áŒ½áŠ“áŠ“ áŠ áŒ¥áŠ“á áŒ á‹ˆáˆˆáŒˆ áŒ áŒˆáŠáŠ á‹˜áŒˆáŠáŠ áŒ£áˆ˜áŠ áŒ£áˆ¨ áŒ¦áˆ›áˆ­ áŒ¦áˆ á‹°áˆ« áŒ¨áˆ˜á‰° áŒ¨áˆ˜á‰°áˆ¨ \\\n","    áŒ¨áˆ˜á‹°á‹° áŒ¨á‰¦á‹° áŒ¨áŠáŒˆá‹˜ áŒ¨á‹°á‹° áŒ¨áˆáŒˆáŒˆ áŒ«á‰°áˆ¨ áŒ­á‰¦ áŒ¸áŠ“ áˆáˆˆáˆ° áˆáˆˆá‰€áŒ  áˆáˆˆáŒ áˆáˆáˆáˆ‹ áˆáˆ¨áŒ… áˆáˆ¨áŒ áŒ  áˆá‰°áŒˆ á”á‹³áŒáŒ‚ á‰°áˆ°áŒˆáˆ°áŒ áŒ®á‰¤ áŒ¨áŠ¨áŠ¨ á‰°áŒ¨áŠ“áŒáˆˆ áŠ®áˆ­áˆ›á‰³ áŒ¨áˆ¨áˆ áŒ¨áˆ¨áŒˆá‹° áŒ§áˆª áŠ áŒ¦áˆˆ áŠ áŒ¤áŠ \\\n","    áŒ¥áŒ‹á‰µ áŒ á‹ˆáˆ¨ áŒ áŠá‰ á‹˜ áŒ¥áŠ•áˆµáˆµ áŒ áˆ¨á‰€ áŒ áˆ¨á‰ƒ áŒ“áˆáˆˆ áŒá‹°áˆáˆ¨ áŠáŒá‹° áŒ‰áŠ•á‰áˆ áŒá‰ áŒá‰  áŒáˆ˜á‹µ áŒáˆˆáˆ˜ áŒá‹µáˆá‰µ áŒ‹áˆ áŒ‹áŒ áˆ¨ áŒ‹áŠ•á‹µá‹« áŒ áˆ¨áŠ•áŒˆáˆ áŒ‹áˆ˜áˆ¨ áŒ‹áˆ¨áŒ  á‹°áŠá‰€áˆ¨ áŒ‰áŒáˆµ áŒ‰áŠ•áŒ‰áŠ• áŒ‰á‰£ áŠ®áˆ¨á•á‰³ áŒ‰áˆ«áŒ… áŒáŒáˆ­ á‰°áŒ‹á‹µáˆ \\\n","    áŒˆá‹°á‹° áŒˆá‹³á‹³ áŠ áŠ•áŒ‹á‹°á‹° áŒˆá‹°á‰¥ áŒ‹á‹¨ áŒˆáŠ“á‹˜á‰  áŠ áŒˆáŠ“á‹˜á‰  áŒˆá‰ á‹¨ áˆáŒá‰£áˆ­ áŒˆáˆ¸áˆ¸ á‹°áˆ­á‰£á‰£ áŒˆáˆ«áˆ« áŒˆáˆ˜áŒ áŒ  áŒˆáˆ˜áˆ¨ áŒáˆáŒáˆ áŠ áŒˆáˆˆá‹°áˆ˜ áŒ…áŠ•áŒáˆ‹ á‹¶áˆˆá‹ á‹³á‹³ áŠ á‹°áŒˆá‹°áŒˆ áŠ áŒá‰ á‹°á‹° á‹°á‹¨áŠ á‹°á‹ˆáˆ¨ á‹°áŠ« á‹°áŠ¨áˆ¨ á‹°á‰ áˆ° á‹°áˆ¨á‹˜ á‹°áˆˆáˆáˆ° \\\n","    á‰°áŠ•á‹°áˆ‹á‰€á‰€ á‰°áŠ•á‹ á‰ áˆ¨áˆ¨ á‹áá‰µ á‹˜áŒ áˆ¨ á‹˜áˆá‰€ á‹˜á‹¨áŠ á‹˜áŠá‹˜áŠ á‹˜áˆ­á á‹á‰…áˆ­ á‹‹áŒ€ á‹‹á‰°á‰° á‰£á‰°áˆˆ á‹‹áˆˆáŒˆ á‹‹áˆ­á‹³ á‹ˆáˆáˆ á‰°á‹ˆáŒ£ á‹ˆáŒ‹áŒˆáŠ• á‹ˆá‹°áˆ¨áŠ› á‹ˆáŠªáˆ á‰°á‹ˆáŠ“á‰ á‹° á‹ˆá‰³á‹ áŠ á‹‹á‰€áˆ¨ á‰°á‹ˆá‰€áˆ¨ á‹ˆáˆ¸á‰£ áŠ áˆ˜áˆáˆ›áˆ á‹ˆáˆ¸áŠ” á‹ˆáˆ°á‹ \\\n","    á‹ˆáˆµá‹‹áˆ³ á‹ˆáˆ°áˆ° á‹ˆáˆ‹á‹ˆáˆˆ á‹ˆáˆ‹áˆ‹ á‹ˆáˆáˆ›áˆ› áŠ³áˆ¸ á‰°áŠ®áˆáˆ° áŠ®áˆáŠ• áŠ®á‰³ áŠ©áˆ© áŒ…áŠ•áŠ• á‰áŠ•áŠ• áŠ®áˆ¨áˆ˜á‰° áŠ©áˆ¨áŒƒ áŠ­áˆµá‰°á‰µ áŠ­áˆªáŠ­ áŠ©áŠ­áŠ’ áŒ­áˆ­á‰µ áŠ áŠ¨áˆáˆˆ áŠ¨áˆáˆ¨áˆ¨ á‰€áˆáˆ¨áˆ¨ áŠ áŠ¨áŠáˆáˆ° áŠ áŒá‰ áŒá‰  áŠ¨áŠá‰°áˆ¨ áŠ­á‰¥áˆ­ áˆáŠ¥áˆáŠ“ áŠ¨áˆ°á‰° áŠ¨áˆ¨áŠ¨áˆ¨ \\\n","    áŠ¨áˆˆáˆ áŠ¨áˆˆá‰ áˆ° áŠ¨áˆˆáˆ° áŠ¥á‹µáˆ­ áŠ áˆáŒˆ áŠ áŒ€áˆˆ áŠ á‹µáˆ­á‰£á‹­ áŠ á‹˜á‰…á‰µ áŠ¥áŠ©á‹« áŠ áŠ•áŒƒ áŠ áŠ“á‹°áˆˆ áŠ áŠ“áŒ áˆ¨ áŠ á‰µáˆ‹áˆµ áŠ á‰°á‰° áŠ á‰£á‹œ áŠ á‰¥áŠá‰µ áŠ á‰£áˆª áŠ á‰ á‰€ áŠ áˆµá‰¤á‹› áˆ›áˆ°áˆµ áŠ áˆ¨áŠ•á‹› áŠ áˆ˜áˆ¨á‰ƒ áŠ áˆˆáˆ˜áŒ  áŠáˆáˆˆáˆˆ áŠ áŠ“áŒ á‰  áŠáŒˆáˆ¨áˆáŒ… áŠáŠ®á‰° \\\n","    áˆ˜áˆ¸áŠ¨ áˆ˜áŠ•á‰†áˆ­ áŠ•á‰…áˆµ á‰¶á‰³áŠ• á‰±á‰£ á‰±áŒƒáˆ­ á‰°áŠ á‰…á‰¦ á‰°áŠáŠ áˆ˜áˆáˆ…á‰…\"\n","    root=root.split()\n","    colum=[\"word\",\"count\"]\n","    lemma=dataset = pd.DataFrame(columns=colum)\n","    for w in root:\n","        lemma.loc[len(lemma.index)]=[w,0]\n","    lemma.to_excel(paths+'/dataset/other/complex_word.xlsx',index=False)\n","\n","spch=open(paths+\"/dataset/stoplist/spchar.txt\",'r',encoding=\"utf-8\").read().split()\n","amharicstop=open(paths+\"/dataset/stoplist/amharic_stop_lists.txt\",'r',encoding=\"utf-8\").read().split()"]},{"cell_type":"code","execution_count":5,"id":"wQflioDU1Pfy","metadata":{"id":"wQflioDU1Pfy","executionInfo":{"status":"ok","timestamp":1725015053705,"user_tz":-180,"elapsed":498,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}}},"outputs":[],"source":["def ComplexityAnotator(text):\n","    sentences=re.split('[?á¢!\\n]', text)\n","    for i in sentences:\n","        if len(i)<250 and len(i)>10:\n","            with open(paths+'/dataset/sentence.txt', 'a',encoding=\"utf-8\") as file:\n","                file.write(i+\"\\n\")\n","\n","    #start extructing text\n","    column=[\"text\",\"label\"]\n","    lemma = pd.DataFrame()\n","    dataset = pd.DataFrame(columns=column)\n","    complx = pd.DataFrame(columns=column)\n","    noncomplx = pd.DataFrame(columns=column)\n","    preproces = pd.DataFrame(columns=column)\n","    newsentenc=\"\"\n","\n","    #saved complex annotated data\n","    if os.path.exists(paths+\"/dataset/dataset.xlsx\"):\n","        complxold = pd.read_excel(paths+\"/dataset/dataset.xlsx\")\n","        os.remove(paths+\"/dataset/dataset.xlsx\")\n","        complx=pd.concat([complxold,complx])\n","\n","    #saved reserved annotated data\n","    if os.path.exists(paths+\"/dataset/other/reserve.xlsx\"):\n","        reserves = pd.read_excel(paths+\"/dataset/other/reserve.xlsx\")\n","        os.remove(paths+\"/dataset/other/reserve.xlsx\")\n","        noncomplx=pd.concat([reserves,noncomplx])\n","\n","    if os.path.exists(paths+\"/dataset/sentence.txt\"):\n","        allfiles = glob.glob(paths+'/dataset/sentence.txt')#most change simple to sentence\n","        df = pd.concat((pd.read_csv(f, header=None, names=[\"text\"]) for f in allfiles))\n","        lemma = pd.read_excel(paths+\"/dataset/other/complex_word.xlsx\")\n","        if df.empty==False:\n","            for sent in df[\"text\"]:\n","                catch=\"\"\n","                rootsent=\"\"\n","                tokens=sent.split()\n","                for words in tokens:\n","                    reslt=\"\"\n","                    if words not in spch:\n","                        wordrt=hm.anal('amh', words, um=True)\n","                        if wordrt!=[]:\n","                            wordlema=wordrt[0]['lemma'].replace(\"|\", \"/\")\n","                            if \"/\" in wordlema:\n","                                reslt = re.search('(.*)/', wordlema)\n","                                reslt=reslt.group(1)\n","                                rootsent+=reslt+\" \"\n","                            else:\n","                                reslt=wordlema\n","                                rootsent=rootsent+\" \"+reslt+\" \"\n","                        else:\n","                            reslt=words\n","                            rootsent=rootsent+\" \"+reslt+\" \"\n","                    else:\n","                        reslt=words\n","                        rootsent=rootsent+\" \"+reslt+\" \"\n","                index=0\n","                for comp in lemma[\"word\"].values:\n","                    if \" \"+comp+\" \" in rootsent:\n","                        catch=\"found\"\n","                        if len(rootsent)<250 and len(rootsent)>10 and lemma.loc[index,'count']<50 and sent not in complx.text.values:\n","                            lemma.loc[index,'count']=lemma.loc[index,'count']+1\n","                            clear_output(wait=True)\n","                            print(comp)\n","                            print(sent)\n","                            newsentenc=\"upgraded\"\n","                            complx.loc[len(complx.index)]=[sent,1]#[rootsent,1]\n","                    index+=1\n","                if catch==\"\" and len(sent)<250 and len(sent)>10 and sent not in noncomplx.text.values:\n","                    noncomplx.loc[len(noncomplx.index)]=[sent,0]\n","        if newsentenc==\"\":\n","            print(\"No new data found sentencess already exist\")\n","\n","    print()\n","    #delete old complex terms  and save new one\n","    if os.path.exists(paths+\"/dataset/other/complex_word.xlsx\"):\n","        os.remove(paths+\"/dataset/other/complex_word.xlsx\")\n","        lemma.to_excel(paths+'/dataset/other/complex_word.xlsx',index=False)\n","\n","    # Balance dataset size\n","    reserve = pd.DataFrame(columns=column)\n","    result=Counter(complx.label.values==1)\n","    comp=result[True]\n","    simp=result[False]\n","    rslt=comp-simp\n","    c=0\n","    if rslt>1:\n","        for i in noncomplx[\"text\"]:\n","            c+=1\n","            if c<rslt:\n","                complx.loc[len(complx.index)]=[i,0]\n","            else:\n","                reserve.loc[len(reserve.index)]=[i,0]\n","    else:\n","        reserve=pd.concat([reserve,noncomplx])\n","    if os.path.exists(paths+\"/dataset/sentence.txt\"):\n","        os.remove(paths+\"/dataset/sentence.txt\")\n","    complx.to_excel(paths+'/dataset/dataset.xlsx',index=False)\n","    reserve.to_excel(paths+'/dataset/other/reserve.xlsx',index=False)\n","    print(str(len(complx))+\" Sentencess are Annotated\")\n","    print(\"Please find the Dataset in path: \"+paths+'/dataset/dataset.xlsx')\n","    print()\n","\n","    # Dataset distribution\n","    result=Counter(complx.label.values==1)\n","    comp=result[True]\n","    simp=result[False]\n","    total=comp+simp\n","    complx=0\n","    simpl=0\n","    if total>0:\n","        complx=round((comp/total)*100,1)\n","        simpl=round((simp/total)*100,1)\n","    print(\"data distribution: complex \"+str(complx)+\"%\"+\" Simple \"+str(simpl)+\"%\")\n","    if complx>55:\n","          print(\"Data imbalancation issue please add more data to balance the distribution\")\n","    print()\n","\n","    #Data Pre-processing\n","    if newsentenc!=\"\":\n","        data=pd.read_excel (paths+'/dataset/dataset.xlsx')\n","        pros=input(\"Do You Want to Pre-process the dataset Y/N\")\n","        print(\"Preprocessing Data Please wait...\")\n","        if pros==\"Y\" or pros==\"y\":\n","            #Remove unexpected char like \\ueff\n","            for indexs, cell_val in enumerate(data[\"text\"].values):\n","                cell_vals=cell_val.split()\n","                cell_val=\"\"\n","                for wrd in cell_vals:\n","                    if wrd not in spch:\n","                        cell_val+=wrd+\" \"\n","                        data.loc[indexs,'text'] = cell_val\n","\n","            #remove special characters\n","            for indexs, cell_val in enumerate(data[\"text\"].values):\n","                for i in spch:\n","                    cell_val=cell_val.replace(i, \"\")\n","                data.loc[indexs,'text'] = cell_val\n","\n","            # remove stopwords\n","            for index, sentence in enumerate(data[\"text\"].values):\n","                sentence=sentence.split()\n","                nonstop_stor=\"\"\n","                for word in sentence:\n","                    if word not in amharicstop:\n","                        nonstop_stor+=word+\" \"\n","                if nonstop_stor!=\"\":\n","                    data.loc[index,'text'] = nonstop_stor\n","            #Normalize text\n","            try:\n","                for index, sentence in enumerate(data[\"text\"].values):\n","                    normalized = normalizer.normalize(sentence)\n","                    data.loc[index,'text'] = normalized\n","            except Exception as err:\n","                  print()\n","\n","            #convert to root\n","            try:\n","                for index, sent in enumerate(data[\"text\"].values):\n","                    rootsent=\"\"\n","                    tokens=sent.split()\n","                    for words in tokens:\n","                        reslt=\"\"\n","                        if words not in spch:\n","                            wordrt=hm.anal('amh', words, um=True)\n","                            if wordrt!=[]:\n","                                wordlema=wordrt[0]['lemma'].replace(\"|\", \"/\")\n","                                if \"/\" in wordlema:\n","                                    reslt = re.search('(.*)/', wordlema)\n","                                    reslt=reslt.group(1)\n","                                    rootsent+=reslt+\" \"\n","                                else:\n","                                    reslt=wordlema\n","                                    rootsent=rootsent+\" \"+reslt+\" \"\n","                            else:\n","                                reslt=words\n","                                rootsent=rootsent+\" \"+reslt+\" \"\n","                        else:\n","                            reslt=words\n","                            rootsent=rootsent+\" \"+reslt+\" \"\n","                    data.loc[index,'text'] = rootsent\n","            except Exception as err:\n","                print()\n","\n","            if os.path.exists(paths+\"/dataset/preprocessed_data.xlsx\"):\n","                preproces = pd.read_excel(paths+\"/dataset/preprocessed_data.xlsx\")\n","                os.remove(paths+\"/dataset/preprocessed_data.xlsx\")\n","                data=pd.concat([preproces,data])\n","                data.drop_duplicates()\n","            data.to_excel(paths+'/dataset/preprocessed_data.xlsx',index=False)\n","            print(\"Please find Preprocessed dataset in path: \"+paths+'/dataset/preprocessed_data.xlsx')\n","\n","        #build new vocabulary\n","        print()\n","        bvcb=input(\"Do You Want to Build vocab for pretrained models Y/N\")\n","        print(\"Building Vocab Please wait...\")\n","        if bvcb==\"Y\" or bvcb==\"y\":\n","            bildvocab()\n","            vocab=open(paths+\"/dataset/vocab.txt\",'r',encoding=\"utf-8\")\n","            vocab=vocab.read()\n","            vocab = vocab.split()\n","            print()\n","            print(\"Building new vocabulary wait...\")\n","            for sent in data[\"text\"]:\n","                sent=sent.split()\n","                for word in sent:\n","                    if word not in vocab:\n","                        with open(paths+'/dataset/vocab.txt', 'a',encoding=\"utf-8\") as file:\n","                            file.write(word+\"\\n\")\n","                        vocab=open(paths+\"/dataset/vocab.txt\",'r',encoding=\"utf-8\")\n","                        vocab=vocab.read()\n","                        vocab = vocab.split()\n","            print()\n","            print(\"Total vocabulary built: \"+str(len(vocab)))\n","            print(\"Please find vocabularies in path: \"+paths+'/dataset/vocab.txt')"]},{"cell_type":"code","execution_count":6,"id":"71175adc","metadata":{"id":"71175adc","executionInfo":{"status":"ok","timestamp":1725015066217,"user_tz":-180,"elapsed":512,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}}},"outputs":[],"source":["def bildvocab():\n","    if not os.path.exists(paths+\"/dataset/vocab.txt\"):\n","        vocab=\"[PAD] [unused0] [unused1] [unused2] [unused3] [unused4] [unused5] [unused6] [unused7] [unused8] [unused9] [unused10] [unused11] [unused12]\\\n","        [unused13] [unused14] [unused15] [unused16] [unused17] [unused18] [unused19] [unused20] [unused21] [unused22] [unused23] [unused24] [unused25] \\\n","        [unused26] [unused27] [unused28] [unused29] [unused30] [unused31] [unused32] [unused33] [unused34] [unused35] [unused36] [unused37] [unused38] \\\n","        [unused39] [unused40] [unused41] [unused42] [unused43] [unused44] [unused45] [unused46] [unused47] [unused48] [unused49] [unused50] [unused51] \\\n","        [unused52] [unused53] [unused54] [unused55] [unused56] [unused57] [unused58] [unused59] [unused60] [unused61] [unused62] [unused63] [unused64] \\\n","        [unused65] [unused66] [unused67] [unused68] [unused69] [unused70] [unused71] [unused72] [unused73] [unused74] [unused75] [unused76] [unused77] \\\n","        [unused78] [unused79] [unused80] [unused81] [unused82] [unused83] [unused84] [unused85] [unused86] [unused87] [unused88] [unused89] [unused90] \\\n","        [unused91] [unused92] [unused93] [unused94] [unused95] [unused96] [unused97] [unused98] [UNK] [CLS] [SEP] [MASK] [unused99] [unused100] [unused101] \\\n","        [unused102] [unused103] [unused104] [unused105] [unused106] [unused107] [unused108] [unused109] [unused110] [unused111] [unused112] [unused113] \\\n","        [unused114] [unused115] [unused116] [unused117] [unused118] [unused119] [unused120] [unused121] [unused122] [unused123] [unused124] [unused125] \\\n","        [unused126] [unused127] [unused128] [unused129] [unused130] [unused131] [unused132] [unused133] [unused134] [unused135] [unused136] [unused137] \\\n","        [unused138] [unused139] [unused140] [unused141] [unused142] [unused143] [unused144] [unused145] [unused146] [unused147] [unused148] [unused149] \\\n","        [unused150] [unused151] [unused152] [unused153] [unused154] [unused155] [unused156] [unused157] [unused158] [unused159] [unused160] [unused161] \\\n","        [unused162] [unused163] [unused164] [unused165] [unused166] [unused167] [unused168] [unused169] [unused170] [unused171] [unused172] [unused173] \\\n","        [unused174] [unused175] [unused176] [unused177] [unused178] [unused179] [unused180] [unused181] [unused182] [unused183] [unused184] [unused185] \\\n","        [unused186] [unused187] [unused188] [unused189] [unused190] [unused191] [unused192] [unused193] [unused194] [unused195] [unused196] [unused197] \\\n","        [unused198] [unused199] [unused200] [unused201] [unused202] [unused203] [unused204] [unused205] [unused206] [unused207] [unused208] [unused209] \\\n","        [unused210] [unused211] [unused212] [unused213] [unused214] [unused215] [unused216] [unused217] [unused218] [unused219] [unused220] [unused221] \\\n","        [unused222] [unused223] [unused224] [unused225] [unused226] [unused227] [unused228] [unused229] [unused230] [unused231] [unused232] [unused233] \\\n","        [unused234] [unused235] [unused236] [unused237] [unused238] [unused239] [unused240] [unused241] [unused242] [unused243] [unused244] [unused245] \\\n","        [unused246] [unused247] [unused248] [unused249] [unused250] [unused251] [unused252] [unused253] [unused254] [unused255] [unused256] [unused257] \\\n","        [unused258] [unused259] [unused260] [unused261] [unused262] [unused263] [unused264] [unused265] [unused266] [unused267] [unused268] [unused269] \\\n","        [unused270] [unused271] [unused272] [unused273] [unused274] [unused275] [unused276] [unused277] [unused278] [unused279] [unused280] [unused281] \\\n","        [unused282] [unused283] [unused284] [unused285] [unused286] [unused287] [unused288] [unused289] [unused290] [unused291] [unused292] [unused293] \\\n","        [unused294] [unused295] [unused296] [unused297] [unused298] [unused299] [unused300] [unused301] [unused302] [unused303] [unused304] [unused305] \\\n","        [unused306] [unused307] [unused308] [unused309] [unused310] [unused311] [unused312] [unused313] [unused314] [unused315] [unused316] [unused317] \\\n","        [unused318] [unused319] [unused320] [unused321] [unused322] [unused323] [unused324] [unused325] [unused326] [unused327] [unused328] [unused329] \\\n","        [unused330] [unused331] [unused332] [unused333] [unused334] [unused335] [unused336] [unused337] [unused338] [unused339] [unused340] [unused341] \\\n","        [unused342] [unused343] [unused344] [unused345] [unused346] [unused347] [unused348] [unused349] [unused350] [unused351] [unused352] [unused353] \\\n","        [unused354] [unused355] [unused356] [unused357] [unused358] [unused359] [unused360] [unused361] [unused362] [unused363] [unused364] [unused365] \\\n","        [unused366] [unused367] [unused368] [unused369] [unused370] [unused371] [unused372] [unused373] [unused374] [unused375] [unused376] [unused377] \\\n","        [unused378] [unused379] [unused380] [unused381] [unused382] [unused383] [unused384] [unused385] [unused386] [unused387] [unused388] [unused389] \\\n","        [unused390] [unused391] [unused392] [unused393] [unused394] [unused395] [unused396] [unused397] [unused398] [unused399] [unused400] [unused401] \\\n","        [unused402] [unused403] [unused404] [unused405] [unused406] [unused407] [unused408] [unused409] [unused410] [unused411] [unused412] [unused413] \\\n","        [unused414] [unused415] [unused416] [unused417] [unused418] [unused419] [unused420] [unused421] [unused422] [unused423] [unused424] [unused425] \\\n","        [unused426] [unused427] [unused428] [unused429] [unused430] [unused431] [unused432] [unused433] [unused434] [unused435] [unused436] [unused437] \\\n","        [unused438] [unused439] [unused440] [unused441] [unused442] [unused443] [unused444] [unused445] [unused446] [unused447] [unused448] [unused449] \\\n","        [unused450] [unused451] [unused452] [unused453] [unused454] [unused455] [unused456] [unused457] [unused458] [unused459] [unused460] [unused461] \\\n","        [unused462] [unused463] [unused464] [unused465] [unused466] [unused467] [unused468] [unused469] [unused470] [unused471] [unused472] [unused473] \\\n","        [unused474] [unused475] [unused476] [unused477] [unused478] [unused479] [unused480] [unused481] [unused482] [unused483] [unused484] [unused485] \\\n","        [unused486] [unused487] [unused488] [unused489] [unused490] [unused491] [unused492] [unused493] [unused494] [unused495] [unused496] [unused497] \\\n","        [unused498] [unused499] [unused500] [unused501] [unused502] [unused503] [unused504] [unused505] [unused506] [unused507] [unused508] [unused509] \\\n","        [unused510] [unused511] [unused512] [unused513] [unused514] [unused515] [unused516] [unused517] [unused518] [unused519] [unused520] [unused521] \\\n","        [unused522] [unused523] [unused524] [unused525] [unused526] [unused527] [unused528] [unused529] [unused530] [unused531] [unused532] [unused533] \\\n","        [unused534] [unused535] [unused536] [unused537] [unused538] [unused539] [unused540] [unused541] [unused542] [unused543] [unused544] [unused545] \\\n","        [unused546] [unused547] [unused548] [unused549] [unused550] [unused551] [unused552] [unused553] [unused554] [unused555] [unused556] [unused557] \\\n","        [unused558] [unused559] [unused560] [unused561] [unused562] [unused563] [unused564] [unused565] [unused566] [unused567] [unused568] [unused569] \\\n","        [unused570] [unused571] [unused572] [unused573] [unused574] [unused575] [unused576] [unused577] [unused578] [unused579] [unused580] [unused581] \\\n","        [unused582] [unused583] [unused584] [unused585] [unused586] [unused587] [unused588] [unused589] [unused590] [unused591] [unused592] [unused593] \\\n","        [unused594] [unused595] [unused596] [unused597] [unused598] [unused599] [unused600] [unused601] [unused602] [unused603] [unused604] [unused605] \\\n","        [unused606] [unused607] [unused608] [unused609] [unused610] [unused611] [unused612] [unused613] [unused614] [unused615] [unused616] [unused617] \\\n","        [unused618] [unused619] [unused620] [unused621] [unused622] [unused623] [unused624] [unused625] [unused626] [unused627] [unused628] [unused629] \\\n","        [unused630] [unused631] [unused632] [unused633] [unused634] [unused635] [unused636] [unused637] [unused638] [unused639] [unused640] [unused641] \\\n","        [unused642] [unused643] [unused644] [unused645] [unused646] [unused647] [unused648] [unused649] [unused650] [unused651] [unused652] [unused653] \\\n","        [unused654] [unused655] [unused656] [unused657] [unused658] [unused659] [unused660] [unused661] [unused662] [unused663] [unused664] [unused665] \\\n","        [unused666] [unused667] [unused668] [unused669] [unused670] [unused671] [unused672] [unused673] [unused674] [unused675] [unused676] [unused677] \\\n","        [unused678] [unused679] [unused680] [unused681] [unused682] [unused683] [unused684] [unused685] [unused686] [unused687] [unused688] [unused689] \\\n","        [unused690] [unused691] [unused692] [unused693] [unused694] [unused695] [unused696] [unused697] [unused698] [unused699] [unused700] [unused701] \\\n","        [unused702] [unused703] [unused704] [unused705] [unused706] [unused707] [unused708] [unused709] [unused710] [unused711] [unused712] [unused713] \\\n","        [unused714] [unused715] [unused716] [unused717] [unused718] [unused719] [unused720] [unused721] [unused722] [unused723] [unused724] [unused725] \\\n","        [unused726] [unused727] [unused728] [unused729] [unused730] [unused731] [unused732] [unused733] [unused734] [unused735] [unused736] [unused737] \\\n","        [unused738] [unused739] [unused740] [unused741] [unused742] [unused743] [unused744] [unused745] [unused746] [unused747] [unused748] [unused749] \\\n","        [unused750] [unused751] [unused752] [unused753] [unused754] [unused755] [unused756] [unused757] [unused758] [unused759] [unused760] [unused761] \\\n","        [unused762] [unused763] [unused764] [unused765] [unused766] [unused767] [unused768] [unused769] [unused770] [unused771] [unused772] [unused773] \\\n","        [unused774] [unused775] [unused776] [unused777] [unused778] [unused779] [unused780] [unused781] [unused782] [unused783] [unused784] [unused785] \\\n","        [unused786] [unused787] [unused788] [unused789] [unused790] [unused791] [unused792] [unused793] [unused794] [unused795] [unused796] [unused797] \\\n","        [unused798] [unused799] [unused800] [unused801] [unused802] [unused803] [unused804] [unused805] [unused806] [unused807] [unused808] [unused809] \\\n","        [unused810] [unused811] [unused812] [unused813] [unused814] [unused815] [unused816] [unused817] [unused818] [unused819] [unused820] [unused821] \\\n","        [unused822] [unused823] [unused824] [unused825] [unused826] [unused827] [unused828] [unused829] [unused830] [unused831] [unused832] [unused833] \\\n","        [unused834] [unused835] [unused836] [unused837] [unused838] [unused839] [unused840] [unused841] [unused842] [unused843] [unused844] [unused845] \\\n","        [unused846] [unused847] [unused848] [unused849] [unused850] [unused851] [unused852] [unused853] [unused854] [unused855] [unused856] [unused857] \\\n","        [unused858] [unused859] [unused860] [unused861] [unused862] [unused863] [unused864] [unused865] [unused866] [unused867] [unused868] [unused869] \\\n","        [unused870] [unused871] [unused872] [unused873] [unused874] [unused875] [unused876] [unused877] [unused878] [unused879] [unused880] [unused881] \\\n","        [unused882] [unused883] [unused884] [unused885] [unused886] [unused887] [unused888] [unused889] [unused890] [unused891] [unused892] [unused893] \\\n","        [unused894] [unused895] [unused896] [unused897] [unused898] [unused899] [unused900] [unused901] [unused902] [unused903] [unused904] [unused905] \\\n","        [unused906] [unused907] [unused908] [unused909] [unused910] [unused911] [unused912] [unused913] [unused914] [unused915] [unused916] [unused917] \\\n","        [unused918] [unused919] [unused920] [unused921] [unused922] [unused923] [unused924] [unused925] [unused926] [unused927] [unused928] [unused929] \\\n","        [unused930] [unused931] [unused932] [unused933] [unused934] [unused935] [unused936] [unused937] [unused938] [unused939] [unused940] [unused941] \\\n","        [unused942] [unused943] [unused944] [unused945] [unused946] [unused947] [unused948] [unused949] [unused950] [unused951] [unused952] [unused953] \\\n","        [unused954] [unused955] [unused956] [unused957] [unused958] [unused959] [unused960] [unused961] [unused962] [unused963] [unused964] [unused965] \\\n","        [unused966] [unused967] [unused968] [unused969] [unused970] [unused971] [unused972] [unused973] [unused974] [unused975] [unused976] [unused977] \\\n","        [unused978] [unused979] [unused980] [unused981] [unused982] [unused983] [unused984] [unused985] [unused986] [unused987] [unused988] [unused989] \\\n","        [unused990] [unused991] [unused992] [unused993]\"\n","        vocab = vocab.split()\n","        with open(paths+'/dataset/vocab.txt', 'a',encoding=\"utf-8\") as file:\n","            for i in vocab:\n","                file.write(i+\"\\n\")"]},{"cell_type":"code","execution_count":7,"id":"83e9ce8b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83e9ce8b","executionInfo":{"status":"ok","timestamp":1725015113054,"user_tz":-180,"elapsed":35478,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}},"outputId":"4a95afe6-a1e7-4c70-e00f-4ea571586e39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Amharic data from amh_lang\n","Loading FSTs for áŠ áˆ›áˆ­áŠ› (analysis/generation) ...\n","No new data found sentencess already exist\n","\n","8 Sentencess are Annotated\n","Please find the Dataset in path: /content/drive/MyDrive/dataset/dataset.xlsx\n","\n","data distribution: complex 75.0% Simple 25.0%\n","Data imbalancation issue please add more data to balance the distribution\n","\n"]}],"source":["texts=open(\"/content/drive/MyDrive/freetext.txt\",'r',encoding=\"utf-8\").read()\n","ComplexityAnotator(texts)"]},{"cell_type":"code","execution_count":8,"id":"9bGklfYB1AXk","metadata":{"id":"9bGklfYB1AXk","executionInfo":{"status":"ok","timestamp":1725015183418,"user_tz":-180,"elapsed":466,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}}},"outputs":[],"source":["def amTextPreprocessing(textss):\n","    colum=[\"text\"]\n","    data = pd.DataFrame(columns=colum)\n","    sentences=re.split('[?á¢!\\n]', textss)\n","    print(\"Preprocessing Data Please wait...\")\n","    for i in sentences:\n","        data.loc[len(data.index)]=[i]\n","    #Remove unexpected char like \\ueff\n","    for indexs, cell_val in enumerate(data[\"text\"].values):\n","        cell_vals=cell_val.split()\n","        cell_val=\"\"\n","        for wrd in cell_vals:\n","            if wrd not in spch:\n","                cell_val+=wrd+\" \"\n","                data.loc[indexs,'text'] = cell_val\n","\n","    #remove special characters\n","    for indexs, cell_val in enumerate(data[\"text\"].values):\n","        for i in spch:\n","            cell_val=cell_val.replace(i, \"\")\n","        data.loc[indexs,'text'] = cell_val\n","\n","    # remove stopwords\n","    for index, sentence in enumerate(data[\"text\"].values):\n","        sentence=sentence.split()\n","        nonstop_stor=\"\"\n","        for word in sentence:\n","            if word not in amharicstop:\n","                nonstop_stor+=word+\" \"\n","        if nonstop_stor!=\"\":\n","            data.loc[index,'text'] = nonstop_stor\n","    #Normalize text\n","    try:\n","        for index, sentence in enumerate(data[\"text\"].values):\n","            normalized = normalizer.normalize(sentence)\n","            data.loc[index,'text'] = normalized\n","    except Exception as err:\n","          print()\n","    #convert to root\n","    try:\n","        for index, sent in enumerate(data[\"text\"].values):\n","            rootsent=\"\"\n","            tokens=sent.split()\n","            for words in tokens:\n","                reslt=\"\"\n","                if words not in spch:\n","                    wordrt=hm.anal('amh', words, um=True)\n","                    if wordrt!=[]:\n","                        wordlema=wordrt[0]['lemma'].replace(\"|\", \"/\")\n","                        if \"/\" in wordlema:\n","                            reslt = re.search('(.*)/', wordlema)\n","                            reslt=reslt.group(1)\n","                            rootsent+=reslt+\" \"\n","                        else:\n","                            reslt=wordlema\n","                            rootsent=rootsent+\" \"+reslt+\" \"\n","                    else:\n","                        reslt=words\n","                        rootsent=rootsent+\" \"+reslt+\" \"\n","                else:\n","                    reslt=words\n","                    rootsent=rootsent+\" \"+reslt+\" \"\n","            data.loc[index,'text'] = rootsent\n","    except Exception as err:\n","        print()\n","\n","    for sent in data[\"text\"]:\n","          if sent !=\"\":\n","            with open(paths+'/dataset/preprocessed_data.txt', 'a',encoding=\"utf-8\") as file:\n","                file.write(sent+\"\\n\")\n","    print(\"please find the preprocesed dataset in path: \"+paths+'dataset/preprocessed_data.txt')"]},{"cell_type":"code","execution_count":9,"id":"e58d555b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e58d555b","executionInfo":{"status":"ok","timestamp":1725015203568,"user_tz":-180,"elapsed":14227,"user":{"displayName":"Gebregziabihier Nigusie","userId":"10219477997913216055"}},"outputId":"319bd892-876b-4899-a4c6-a719bcf0fafc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing Data Please wait...\n","\n","please find the preprocesed dataset in path: /content/drive/MyDrivedataset/preprocessed_data.txt\n"]}],"source":["texts=open(\"/content/drive/MyDrive/freetext.txt\",'r',encoding=\"utf-8\").read()\n","amTextPreprocessing(texts)"]},{"cell_type":"code","execution_count":null,"id":"37229669","metadata":{"id":"37229669"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1odbcqMyffuKGT2Lg5vGLt5RHIH_g-GN5","timestamp":1682685827215}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}